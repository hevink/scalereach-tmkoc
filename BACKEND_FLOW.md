# ScaleReach Backend â€” Complete Flow Documentation

> For any new developer joining the team. This doc explains every backend flow,
> every conditional branch, and which worker handles what.

---

## ðŸ“¦ Tech Stack (Quick Reference)

| Layer             | Tech                         |
| ----------------- | ---------------------------- |
| HTTP Framework    | Hono (Bun runtime)           |
| Queue System      | BullMQ + Redis               |
| Database          | PostgreSQL (Drizzle ORM)     |
| File Storage      | Cloudflare R2                |
| Video Download    | yt-dlp                       |
| Video Processing  | FFmpeg                       |
| Transcription     | Deepgram                     |
| AI Clip Detection | Gemini                       |
| Translation       | DeepL                        |
| TTS (Dubbing)     | ElevenLabs / other providers |
| Error Tracking    | Sentry                       |

---

## ðŸ—ºï¸ Big Picture â€” All Queues & Workers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        BullMQ Queues                        â”‚
â”‚                                                             â”‚
â”‚  video-processing  â†’  [Video Worker]                        â”‚
â”‚  clip-generation   â†’  [Clip Worker]                         â”‚
â”‚  video-translation â†’  [Translation Worker]                  â”‚
â”‚  voice-dubbing     â†’  [Dubbing Worker]                      â”‚
â”‚  smart-crop        â†’  [Smart Crop Worker]                   â”‚
â”‚  social-posting    â†’  [Social Worker]                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1ï¸âƒ£ VIDEO PROCESSING WORKER

**File:** `src/jobs/video.worker.ts`  
**Queue:** `video-processing`  
**Concurrency:** 2  
**Retries:** 2 (exponential backoff, 5s base)

This is the **entry point** for every video. It runs first before any clips are made.

---

### ðŸ“¥ Source Type Branch

```
User submits video
        â”‚
        â”œâ”€â”€ sourceType === "youtube"
        â”‚         â””â”€â”€ processYouTubeVideo()
        â”‚
        â””â”€â”€ sourceType === "upload"
                  â””â”€â”€ processUploadedVideo()
```

---

### ðŸŽ¬ Flow A â€” YouTube Video

```
[START] User submits YouTube URL
        â”‚
        â–¼
[DB] Load video record + videoConfig
        â”‚
        â–¼
[RESUME CHECK] Has transcript already in DB?
        â”‚
        â”œâ”€â”€ YES (transcript exists) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Skip download + transcription entirely                            â”‚
        â”‚   Jump straight to â†’ [AI VIRAL DETECTION]                          â”‚
        â”‚                                                                     â”‚
        â”œâ”€â”€ PARTIAL (storageKey exists, no transcript) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
        â”‚   Skip download, audio already in R2                               â”‚â”‚
        â”‚   Jump to â†’ [TRANSCRIPTION]                                        â”‚â”‚
        â”‚                                                                     â”‚â”‚
        â””â”€â”€ NONE (fresh job) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚â”‚
                                                                          â”‚  â”‚â”‚
                                                                          â–¼  â–¼â–¼
                                                                [DOWNLOAD AUDIO]
                                                          YouTubeService.streamAudio()
                                                          yt-dlp streams audio only
                                                          timeframeStart/End applied if set
                                                                          â”‚
                                                                          â–¼
                                                                 [UPLOAD TO R2]
                                                          R2Service.uploadFromStream()
                                                          storageKey saved to DB
                                                                          â”‚
                                                                          â–¼
                                                                 [TRANSCRIPTION]
                                                          DeepgramService.transcribeFromUrl()
                                                          language: from config or auto-detect
                                                          words[] with timestamps saved to DB
                                                                          â”‚
                                                                          â–¼
                                                             [AI VIRAL DETECTION]
                                                          ViralDetectionService.detectViralClips()
                                                          Gemini AI analyzes transcript
                                                          Returns: clips with scores, hooks,
                                                          emotions, recommended platforms
                                                                          â”‚
                                                                          â–¼
                                                         [videoConfig.skipClipping?]
                                                                  â”‚
                                                         YES â”€â”€â”€â”€â”€â”¤
                                                         Mark video "completed", done
                                                                  â”‚
                                                         NO â”€â”€â”€â”€â”€â”€â”¤
                                                                  â–¼
                                                     [SAVE CLIPS TO DB + QUEUE GENERATION]
                                                     For each detected clip:
                                                       1. Insert viralClip record
                                                       2. ClipCaptionModel.create() (words + style)
                                                       3. addClipGenerationJob() â†’ clip-generation queue
```

---

### ðŸ“¤ Flow B â€” Uploaded Video

```
[START] User uploads video file (already in R2)
        â”‚
        â–¼
[DB] Load video record + videoConfig
        â”‚
        â–¼
[METADATA] FFmpegService.getVideoMetadata()
  - Get duration, width, height
  - Validate against plan limits
  - Deduct minutes from workspace balance
        â”‚
        â–¼
[THUMBNAIL] FFmpegService.generateThumbnail()
  - Extract frame at 1s
  - Upload to R2
  - Non-fatal: continues even if fails
        â”‚
        â–¼
[AUDIO EXTRACT] FFmpegService.extractAudioToR2()
  - FFmpeg strips video, outputs AAC audio
  - Streams directly to R2 (no disk)
        â”‚
        â–¼
[TRANSCRIPTION] DeepgramService.transcribeFromUrl()
  - Same as YouTube flow above
        â”‚
        â–¼
[AI VIRAL DETECTION] â†’ same as YouTube flow above
        â”‚
        â–¼
[SAVE CLIPS + QUEUE] â†’ same as YouTube flow above
```

---

### âš™ï¸ videoConfig Options That Change the Flow

| Config Option                   | Effect                                                          |
| ------------------------------- | --------------------------------------------------------------- |
| `skipClipping: true`            | Stops after transcription, no clips created                     |
| `timeframeStart / timeframeEnd` | Only processes that portion of the video                        |
| `language`                      | Forces Deepgram to use specific language instead of auto-detect |
| `clipDurationMin / Max`         | Tells Gemini min/max clip length                                |
| `genre`                         | Genre hint to Gemini for better clip detection                  |
| `clipType`                      | `"viral-clips"` or `"highlights"` â€” changes Gemini prompt       |
| `customPrompt`                  | Extra instructions injected into Gemini prompt                  |
| `enableCaptions`                | If false, clips queued without caption data                     |
| `enableIntroTitle`              | If false, no intro title overlay on clips                       |
| `enableSplitScreen`             | Triggers split-screen background video selection                |
| `splitScreenBgVideoId`          | Specific background video(s) â€” or random if empty               |
| `splitRatio`                    | Top/bottom split percentage (default 50)                        |
| `captionTemplateId`             | Which caption style template to use                             |
| `aspectRatio`                   | `"9:16"` / `"16:9"` / `"1:1"`                                   |
| `backgroundStyle`               | `blur` / `black` / `white` / gradients / `mirror` / `zoom`      |

---

### ðŸ”„ Resume Logic (Crash Recovery)

The video worker checks what's already done before starting:

```
Job starts (could be a retry after crash)
        â”‚
        â”œâ”€â”€ transcript in DB?     â†’ skip to Gemini AI step
        â”œâ”€â”€ storageKey in DB?     â†’ skip to Deepgram step
        â””â”€â”€ nothing?              â†’ full flow from scratch
```

Minutes are only refunded if the transcript was **never saved** (meaning real work wasn't done).

---

## 2ï¸âƒ£ CLIP GENERATION WORKER

**File:** `src/jobs/clip.worker.ts`  
**Queue:** `clip-generation`  
**Concurrency:** 2  
**Retries:** 5 (exponential backoff, 5s base)

Each clip detected by Gemini gets its own job here. This is the **heaviest FFmpeg work**.

---

### Main Flow

```
[START] ClipGenerationJobData received
        â”‚
        â–¼
[STATUS] Mark clip as "generating"
        â”‚
        â–¼
[TRANSLATION CHECK] targetLanguage set?
        â”‚
        â”œâ”€â”€ YES â†’ fetch translated captions from DB
        â”‚         apply language-specific style overrides
        â”‚
        â””â”€â”€ NO  â†’ use original captions
        â”‚
        â–¼
[VALIDATE] ClipGeneratorService.validateOptions()
  checks: times, duration (5s-180s), aspect ratio, quality, source
        â”‚
        â–¼
[GENERATE CLIP] ClipGeneratorService.generateClip()
  (see detailed breakdown below)
        â”‚
        â–¼
[SMART CROP CHECK] smartCropEnabled?
        â”‚
        â”œâ”€â”€ YES â†’ run Python face detection sidecar (inline, not queued)
        â”‚         apply FFmpeg crop based on result
        â”‚         (see Smart Crop section below)
        â”‚
        â””â”€â”€ NO  â†’ skip
        â”‚
        â–¼
[THUMBNAIL] ClipGeneratorService.generateThumbnail()
  FFmpeg extracts frame at 1s â†’ upload to R2
  Non-fatal: continues even if fails
        â”‚
        â–¼
[STATUS] Mark clip as "ready"
  storageKey, storageUrl, rawStorageKey, rawStorageUrl, thumbnailKey saved
        â”‚
        â–¼
[EMAIL CHECK] Are ALL clips for this video now ready?
        â”‚
        â”œâ”€â”€ YES â†’ send "all clips ready" email to user
        â””â”€â”€ NO  â†’ skip (other clips still processing)
```

---

### ClipGeneratorService.generateClip() â€” Internal Steps

```
[STEP 1] Download source segment ONCE to temp file
        â”‚
        â”œâ”€â”€ sourceType === "youtube"
        â”‚   â””â”€â”€ yt-dlp --download-sections *start-end
        â”‚       with --force-keyframes-at-cuts (disabled on retry if code 222)
        â”‚       retries up to 3x with exponential backoff
        â”‚
        â””â”€â”€ sourceType === "upload"
            â””â”€â”€ FFmpeg -ss {start} -t {duration} -c copy
                (stream copy, no re-encode = fast)
        â”‚
        â–¼
[STEP 2] Split-screen background setup
        â”‚
        â”œâ”€â”€ splitScreen enabled?
        â”‚   â””â”€â”€ download background video from R2 to temp file
        â”‚
        â””â”€â”€ no split-screen â†’ skip
        â”‚
        â–¼
[STEP 3] Generate RAW clip (no captions, no intro title)
        â”‚
        â”œâ”€â”€ WITH split-screen
        â”‚   â””â”€â”€ convertWithSplitScreen() â€” single FFmpeg pass:
        â”‚       main video top + background video bottom
        â”‚       â†’ vstack â†’ output file
        â”‚
        â””â”€â”€ WITHOUT split-screen
            â””â”€â”€ convertAspectRatioFile() â€” single FFmpeg pass:
                applies backgroundStyle filter
                â†’ output file
        â”‚
        â–¼
[STEP 4] Generate CAPTIONED clip
        â”‚
        â”œâ”€â”€ NO captions, no introTitle, no emojis?
        â”‚   â””â”€â”€ reuse raw buffer (ZERO extra encoding)
        â”‚
        â””â”€â”€ HAS captions / introTitle / emojis
            â”‚
            â–¼
            [BUILD ASS SUBTITLES FILE]
            generateASSSubtitles()
            - groups words into lines (wordsPerLine setting)
            - applies animation style:
              â”œâ”€â”€ "karaoke"      â†’ each word highlighted during its time
              â”œâ”€â”€ "word-by-word" â†’ words appear one by one
              â”œâ”€â”€ "bounce"       â†’ scale animation per word
              â”œâ”€â”€ "fade"         â†’ line fades in
              â””â”€â”€ "none"         â†’ whole line appears at once
            - adds introTitle overlay (first 3s, fade in/out)
            - font sizes scaled to output resolution
            â”‚
            â–¼
            [FFmpeg single pass: aspect ratio + captions]
            â”œâ”€â”€ WITH split-screen â†’ convertWithSplitScreen() + subtitlesPath
            â””â”€â”€ WITHOUT split-screen â†’ convertAspectRatioFile() + subtitlesPath
        â”‚
        â–¼
[STEP 5] Upload BOTH versions to R2
        â”œâ”€â”€ captioned clip  â†’ storageKey   (what user downloads/shares)
        â””â”€â”€ raw clip        â†’ rawStorageKey (used for re-editing captions later)
```

---

### Background Style Decision Tree (Vertical Clips Only)

```
backgroundStyle
        â”‚
        â”œâ”€â”€ "blur"              â†’ blurred + darkened zoomed background
        â”œâ”€â”€ "black"             â†’ solid black background
        â”œâ”€â”€ "white"             â†’ solid white background
        â”œâ”€â”€ "gradient-ocean"    â†’ blue gradient (#1CB5E0 â†’ #000851)
        â”œâ”€â”€ "gradient-midnight" â†’ dark blue gradient (#4b6cb7 â†’ #182848)
        â”œâ”€â”€ "gradient-sunset"   â†’ orange gradient (#FF512F â†’ #F09819)
        â”œâ”€â”€ "mirror"            â†’ video mirrored top+bottom as background
        â””â”€â”€ "zoom"              â†’ zoomed-in dark version as background
```

---

### Quality & Encoding Settings

```
quality
        â”‚
        â”œâ”€â”€ "720p" / "1080p"   â†’ preset: ultrafast, CRF: 22 (fast, less CPU)
        â””â”€â”€ "2k" / "4k"        â†’ preset: medium,    CRF: 18 (better quality)
```

> Plan limits: free/starter â†’ max 1080p. Pro â†’ up to 4k.  
> Split-screen clips always capped at 1080p regardless of plan.

---

## 3ï¸âƒ£ SMART CROP WORKER

**File:** `src/jobs/smart-crop.worker.ts`  
**Queue:** `smart-crop`  
**Concurrency:** 1  
**Retries:** 2

AI face-detection reframing. Converts 16:9 â†’ 9:16 by tracking faces.

> **Note:** Smart crop can be triggered TWO ways:
>
> 1. **Inline** inside `clip.worker.ts` (if `smartCropEnabled` on the clip job)
> 2. **Standalone queue** via `smart-crop` queue (for on-demand reframe after clip is ready)

---

### Flow

```
[START] Raw clip ready in R2
        â”‚
        â–¼
[STATUS] Mark smartCropStatus = "processing"
        â”‚
        â–¼
[PYTHON SIDECAR] smart_crop.py
  - Downloads video from signed R2 URL
  - Runs face detection frame by frame
  - Outputs: {clipId}_coords.json
        â”‚
        â–¼
[READ RESULT] Parse coords.json
        â”‚
        â”œâ”€â”€ mode === "skip"
        â”‚   No face detected
        â”‚   smartCropStatus = "skipped"
        â”‚   Keep original clip as-is
        â”‚
        â”œâ”€â”€ mode === "split"
        â”‚   Screen + face cam detected (e.g. screen recording with PiP)
        â”‚   FFmpegService.applySplitScreen()
        â”‚   Top = screen content, Bottom = face cam
        â”‚
        â”œâ”€â”€ mode === "mixed"
        â”‚   Some segments have face, some don't
        â”‚   FFmpegService.applyMixedCrop()
        â”‚   Processes each segment separately â†’ concat
        â”‚
        â””â”€â”€ mode === "crop"
            Standard face tracking crop
            FFmpegService.applySmartCrop()
            Uses sendcmd for per-frame crop coordinates
        â”‚
        â–¼
[STATUS] smartCropStatus = "done"
  smartCropStorageKey + smartCropStorageUrl saved to DB
```

---

## 4ï¸âƒ£ TRANSLATION WORKER

**File:** `src/jobs/translation.worker.ts`  
**Queue:** `video-translation`  
**Concurrency:** 1 (API-bound, not CPU-bound)  
**Retries:** 3

Translates the video transcript and generates translated caption timing for all clips.

```
[START] User requests translation (source â†’ target language)
        â”‚
        â–¼
[STATUS] translationStatus = "translating"
        â”‚
        â–¼
[FETCH] Load transcriptWords from video DB record
        â”‚
        â–¼
[TRANSLATE] TranslationService.translateTranscript()
  - Sends full transcript to DeepL
  - Re-aligns word timestamps to translated text
  - Returns: translatedText + translatedWords[]
        â”‚
        â–¼
[SAVE] Store translated transcript in DB
        â”‚
        â–¼
[CLIP CAPTIONS] For each clip in the video:
  - Filter translatedWords within clip time range
  - Apply language-specific style overrides
    (e.g. Arabic â†’ right-to-left, CJK â†’ different font)
  - TranslationModel.saveClipCaptions()
        â”‚
        â–¼
[STATUS] translationStatus = "completed"
```

> When a user later regenerates a clip with `targetLanguage` set,
> the clip worker fetches these pre-translated captions and burns them in.

---

## 5ï¸âƒ£ DUBBING WORKER

**File:** `src/jobs/dubbing.worker.ts`  
**Queue:** `voice-dubbing`  
**Concurrency:** 1  
**Retries:** 2

Full AI voice dubbing pipeline. Requires translation to exist first.

```
[START] DubbingJobData (requires translationId)
        â”‚
        â–¼
[STATUS] dubbingStatus = "generating_tts"
        â”‚
        â–¼
[FETCH] Load translatedWords from translation record
        â”‚
        â–¼
[SEGMENT] Group words into sentence segments
  Splits on: sentence-ending punctuation (. ! ?) or word gap > 1s
        â”‚
        â–¼
[TTS LOOP] For each segment:
  TTSService.generateSegment()
  - Provider: ElevenLabs or configured ttsProvider
  - Voice: user-selected voiceId + settings
  â†“
  AudioMixingService.getAudioDuration() on output
  â†“
  AudioMixingService.timeStretch()
  - Stretches/compresses TTS audio to match original timing
        â”‚
        â–¼
[CONCATENATE] AudioMixingService.concatenateWithTiming()
  Joins all segments with silence padding to match full video duration
        â”‚
        â–¼
[UPLOAD] TTS-only track â†’ R2 (dubbing/{videoId}/{dubbingId}-tts.mp3)
        â”‚
        â–¼
[AUDIO MIX] audioMode branch:
        â”‚
        â”œâ”€â”€ "replace" â†’ use TTS audio as-is (original audio fully replaced)
        â”‚
        â””â”€â”€ "duck"    â†’ AudioMixingService.mixAudio()
                        Lower original audio volume to duckVolume%
                        TTS voice plays on top
        â”‚
        â–¼
[UPLOAD] Mixed audio â†’ R2 (dubbing/{videoId}/{dubbingId}-mixed.aac)
        â”‚
        â–¼
[CLIP SLICES] For each clip:
  AudioMixingService.sliceAudio(mixedAudio, startTime, endTime)
  Upload per-clip audio â†’ R2
  Save clipAudio record to DB
        â”‚
        â–¼
[STATUS] dubbingStatus = "completed"
```

> When user exports a clip with dubbing, the clip worker uses the
> pre-sliced per-clip audio instead of the original video audio.

---

## 6ï¸âƒ£ SOCIAL POSTING WORKER

**File:** `src/jobs/social.worker.ts`  
**Queue:** `social-posting`  
**Concurrency:** default  
**Retries:** 3

Handles posting clips to TikTok, Instagram, YouTube Shorts etc.

```
[START] SocialPostingJobData
  (postId, clipId, platform, caption, hashtags)
        â”‚
        â–¼
[FETCH] Load clip storageUrl from DB
        â”‚
        â–¼
[POST] Platform-specific API call
  â”œâ”€â”€ TikTok   â†’ TikTok Content Posting API
  â”œâ”€â”€ Instagram â†’ Meta Graph API (Reels)
  â””â”€â”€ YouTube  â†’ YouTube Data API (Shorts)
        â”‚
        â–¼
[STATUS] Update post status in DB
  success â†’ "published" with platform post ID
  failure â†’ "failed" with error message
```

---

## ðŸ”— Full End-to-End Flow (Happy Path â€” YouTube, All Features ON)

```
1.  User submits YouTube URL + config
          â”‚
2.  API adds job â†’ video-processing queue (priority by plan)
          â”‚
3.  Video Worker:
      yt-dlp downloads audio
      â†’ R2 upload
      â†’ Deepgram transcription
      â†’ Gemini detects N viral clips
      â†’ Saves N clip records (status: "detected")
      â†’ Queues N jobs â†’ clip-generation queue
          â”‚
4.  Clip Worker (runs N jobs, concurrency=2):
      For each clip:
        yt-dlp downloads video segment
        â†’ FFmpeg: aspect ratio + background style â†’ raw clip
        â†’ FFmpeg: raw clip + ASS captions â†’ captioned clip
        â†’ Upload both to R2
        â†’ Generate thumbnail â†’ R2
        â†’ [if smartCropEnabled] Python face detection â†’ FFmpeg reframe
        â†’ Mark clip "ready"
          â”‚
5.  All N clips ready â†’ Email sent to user
          â”‚
--- Optional flows user can trigger after this point ---
          â”‚
6.  [OPTIONAL] User requests Translation:
      â†’ video-translation queue
      â†’ DeepL translates transcript
      â†’ Saves translated captions per clip
          â”‚
7.  [OPTIONAL] User requests Dubbing:
      â†’ voice-dubbing queue
      â†’ ElevenLabs TTS per segment
      â†’ Time-stretch to match timing
      â†’ Mix with original audio
      â†’ Save per-clip dubbed audio
          â”‚
8.  [OPTIONAL] User regenerates clip with translated captions:
      â†’ New job â†’ clip-generation queue
      â†’ targetLanguage set â†’ fetches translated captions
      â†’ Burns translated captions into new clip
          â”‚
9.  [OPTIONAL] User schedules social post:
      â†’ social-posting queue
      â†’ Posts to TikTok / Instagram / YouTube
```

---

## ðŸ—„ï¸ Job Priority System

```
Plan    â”‚ Priority
â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€
pro     â”‚    1     â† processed first
starter â”‚    2
free    â”‚    3     â† processed last
```

Controlled by `getPlanPriority()` in `queue.ts`. Applied to both `video-processing` and `clip-generation` queues.

---

## ðŸ” Retry & Failure Handling

| Worker         | Max Retries | Notes                                                                                                                      |
| -------------- | ----------- | -------------------------------------------------------------------------------------------------------------------------- |
| Video Worker   | 2           | Only marks failed on last attempt. Minutes refunded if transcript never saved. R2 cleaned up only if uploaded in this run. |
| Clip Worker    | 5           | yt-dlp 202/222 errors retried with backoff. `forceKeyframes` disabled after code 222.                                      |
| Smart Crop     | 2           | Python sidecar failure = non-fatal (clip stays as-is)                                                                      |
| Translation    | 3           | Full retry                                                                                                                 |
| Dubbing        | 2           | Full retry                                                                                                                 |
| Social Posting | 3           | Full retry                                                                                                                 |

---

## ðŸ“ Key File Map

```
src/
â”œâ”€â”€ jobs/
â”‚   â”œâ”€â”€ queue.ts                   â† All queue definitions, job interfaces, priorities
â”‚   â”œâ”€â”€ video.worker.ts            â† YouTube + Upload video processing
â”‚   â”œâ”€â”€ clip.worker.ts             â† FFmpeg clip generation + smart crop inline
â”‚   â”œâ”€â”€ smart-crop.worker.ts       â† Standalone AI reframe worker
â”‚   â”œâ”€â”€ translation.worker.ts      â† DeepL translation
â”‚   â”œâ”€â”€ dubbing.worker.ts          â† TTS + audio mixing
â”‚   â””â”€â”€ social.worker.ts           â† Social platform posting
â”‚
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ ffmpeg.service.ts          â† FFmpeg wrappers (extract, thumbnail, smart crop apply)
â”‚   â”œâ”€â”€ clip-generator.service.ts  â† Full clip generation pipeline, ASS subtitle builder
â”‚   â”œâ”€â”€ youtube.service.ts         â† yt-dlp wrapper, audio streaming
â”‚   â”œâ”€â”€ deepgram.service.ts        â† Transcription
â”‚   â”œâ”€â”€ viral-detection.service.ts â† Gemini AI clip detection
â”‚   â”œâ”€â”€ translation.service.ts     â† DeepL + language style overrides
â”‚   â”œâ”€â”€ tts.service.ts             â† TTS provider abstraction
â”‚   â”œâ”€â”€ audio-mixing.service.ts    â† FFmpeg audio concat/mix/stretch/slice
â”‚   â”œâ”€â”€ r2.service.ts              â† Cloudflare R2 upload/download/sign
â”‚   â””â”€â”€ split-screen-compositor.service.ts â† Background video download + input args
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ video-config.model.ts      â† User video settings (aspect ratio, captions, etc.)
â”‚   â”œâ”€â”€ clip-caption.model.ts      â† Caption words + style stored per clip
â”‚   â”œâ”€â”€ translation.model.ts       â† Translation records + clip captions
â”‚   â””â”€â”€ dubbing.model.ts           â† Dubbing records + clip audio
â”‚
â””â”€â”€ scripts/
    â””â”€â”€ smart_crop.py              â† Python face detection sidecar (outputs coords.json)
```

---

## âš ï¸ Known Gotchas for New Devs

1. **yt-dlp exit code 222** â€” caused by `--force-keyframes-at-cuts` on certain streams.
   The retry logic automatically disables it. Do NOT remove the retry logic.

2. **Clip worker concurrency > 2** â€” can cause exit code 202 errors when multiple
   yt-dlp processes compete. Currently capped at 2. Retry handles it but bump carefully.

3. **Smart crop inline vs queued** â€” smart crop can run inside the clip worker (inline)
   OR as a separate queue job. Both paths exist. Don't add it twice.

4. **Raw clip = no captions** â€” `rawStorageKey` is intentionally caption-free.
   It's used when the user edits captions in the UI and triggers a re-render.
   The re-render job takes the raw clip and burns new captions.

5. **Minutes deducted at video level, not clip level** â€” `creditCost: 0` on clip jobs
   is intentional. Minutes already deducted when video was submitted.

6. **Resume logic only exists in video worker** â€” the clip worker always starts fresh.
   If a clip job fails partway, it re-encodes from scratch.

7. **Signed URLs expire in 1 hour** â€” all `getSignedDownloadUrl()` calls use `3600s`.
   If a job takes longer (large video + slow encode), the URL can expire mid-job.
   FFmpeg will fail silently with a 403. Use `removeOnFail` logs to catch this.

8. **`QUEUE_PREFIX` env var** â€” set `QUEUE_PREFIX=local` in `.env.local` to prevent
   your local worker from picking up production jobs from the same Redis.
